# -*- coding: utf-8 -*-
"""Framework 2: LCGAN for facial expression conversion.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYn_iK6AhMczGc5KFIz4kX3Aq1Gwugx1
"""

# Framework 2: LCGAN for facial expression conversion

# LCGAN-style landmark-conditional editing (smile (F1)-> sad(F2)) using dlib 68-pt landmarks

import os, bz2, shutil
import cv2, dlib
import numpy as np
import pandas as pd
from pathlib import Path

# ---------- paths ----------
IMAGE_PATH = "/content/drive/MyDrive/facial lanadmarks/I1.jpg"
# Use either .dat or .dat.bz2; loader below handles both
PREDICTOR_PATH = "/content/drive/MyDrive/facial lanadmarks/shape_predictor_68_face_landmarks.dat"
OUT_SAD_IMG   = "/content/drive/MyDrive/facial lanadmarks/sad_face_lcgan.png"
OUT_DISTS_CSV = "/content/drive/MyDrive/facial lanadmarks/lcgan_distance_report.csv"
OUT_32_CSV    = "/content/drive/MyDrive/facial lanadmarks/custom32_face_eyes_after.csv"

# ---------- definitions ----------
def ensure_shape_predictor(dat_or_bz2_path: str) -> dlib.shape_predictor:
    p = dat_or_bz2_path
    if not os.path.exists(p):
        alt = (p + ".bz2") if not p.endswith(".bz2") else p[:-4]
        if os.path.exists(alt):
            p = alt
    if not os.path.exists(p):
        raise FileNotFoundError(f"Predictor not found: {dat_or_bz2_path}")

    if p.endswith(".bz2"):
        out_dat = p[:-4]
        if not os.path.exists(out_dat):
            print(f"Decompressing {p} -> {out_dat} ...")
            with bz2.open(p, "rb") as f_in, open(out_dat, "wb") as f_out:
                shutil.copyfileobj(f_in, f_out)
        p = out_dat

    if os.path.getsize(p) < 50_000_000:  # 68-pt file ~99MB
        raise RuntimeError("Predictor file too small; re-download the official 68-pt model.")
    return dlib.shape_predictor(p)

def shape_to_np(shape: dlib.full_object_detection) -> np.ndarray:
    pts = np.zeros((shape.num_parts, 2), dtype=np.float32)
    for i in range(shape.num_parts):
        pts[i] = (shape.part(i).x, shape.part(i).y)
    return pts

def one_based_pair(a, b):  # convert your 1-based indexes to 0-based
    return (a - 1, b - 1)

def pair_distances(pts, pair_list, names):
    d = {}
    for name, (i, j) in zip(names, pair_list):
        d[name] = float(np.linalg.norm(pts[i] - pts[j]))
    return d

def accumulate_displacements(n_points):
    return np.zeros((n_points, 2), dtype=np.float32), np.zeros(n_points, dtype=np.float32)

def shrink_pair(pts, i, j, factor, disp_sum, disp_w):
    """
    Move i and j symmetrically towards each other s.t. new_dist = factor * old_dist.
    Accumulates displacements so multiple constraints can be averaged.
    """
    factor = float(factor)
    if factor >= 0.999:
        return
    vi = pts[i].copy(); vj = pts[j].copy()
    vec = vj - vi
    d = np.linalg.norm(vec)
    if d < 1e-6:
        return
    pull = 0.5 * (1.0 - factor) * d
    dir_ = vec / d
    di = +pull * dir_
    dj = -pull * dir_
    disp_sum[i] += di; disp_w[i] += 1.0
    disp_sum[j] += dj; disp_w[j] += 1.0

def apply_displacements(pts, disp_sum, disp_w):
    out = pts.copy()
    nz = disp_w > 0
    out[nz] = pts[nz] + (disp_sum[nz] / disp_w[nz, None])
    return out

# ---- warp distances ----
def triangle_area(tri):  # tri: (3,2)
    a = tri[1] - tri[0]
    b = tri[2] - tri[0]
    return 0.5 * abs(a[0]*b[1] - a[1]*b[0])

def triangle_warp(img, src_pts, dst_pts, triangles):
    """
    src_pts, dst_pts: (N,2) float32
    triangles: array of indices shape (M,3)
    - guards for degenerate triangles and zero-sized rects
    - blends warped patches
    """
    h, w = img.shape[:2]
    out = np.zeros_like(img)

    for simplex in triangles:
        src_tri = np.asarray(src_pts[simplex], dtype=np.float32)  # (3,2)
        dst_tri = np.asarray(dst_pts[simplex], dtype=np.float32)  # (3,2)

        if triangle_area(src_tri) < 1e-3 or triangle_area(dst_tri) < 1e-3:
            continue

        x1, y1, w1, h1 = cv2.boundingRect(src_tri)
        x2, y2, w2, h2 = cv2.boundingRect(dst_tri)
        if w1 < 1 or h1 < 1 or w2 < 1 or h2 < 1:
            continue

        src_off = (src_tri - np.array([[x1, y1]], dtype=np.float32)).reshape(3, 2)
        dst_off = (dst_tri - np.array([[x2, y2]], dtype=np.float32)).reshape(3, 2)

        M = cv2.getAffineTransform(src_off, dst_off)

        patch = img[y1:y1+h1, x1:x1+w1]
        warped = cv2.warpAffine(patch, M, (w2, h2),
                                flags=cv2.INTER_LINEAR,
                                borderMode=cv2.BORDER_REFLECT_101)

        mask = np.zeros((h2, w2), dtype=np.uint8)
        cv2.fillConvexPoly(mask, np.int32(dst_off), 255)

        roi = out[y2:y2+h2, x2:x2+w2]
        warped_masked = cv2.bitwise_and(warped, warped, mask=mask)
        bg = cv2.bitwise_and(roi, roi, mask=cv2.bitwise_not(mask))
        out[y2:y2+h2, x2:x2+w2] = cv2.add(bg, warped_masked)


    holes = (out == 0).all(axis=2)
    out[holes] = img[holes]
    return out

def delaunay_triangles(pts):
    """Return triangle indices via SciPy if available, else OpenCV Subdiv2D."""
    try:
        from scipy.spatial import Delaunay
        tri = Delaunay(np.asarray(pts, dtype=np.float64))
        return tri.simplices
    except Exception:
        # Fallback to OpenCV Subdiv2D
        h = int(np.max(pts[:,1]) + 2)
        w = int(np.max(pts[:,0]) + 2)
        rect = (0, 0, w, h)
        subdiv = cv2.Subdiv2D(rect)
        for (x, y) in pts:
            subdiv.insert((float(x), float(y)))
        triangle_list = subdiv.getTriangleList()
        triangles = []
        # Map triangle vertices back to nearest landmark indices
        for t in triangle_list:
            pts_tri = np.array([[t[0], t[1]], [t[2], t[3]], [t[4], t[5]]], dtype=np.float32)
            idxs = []
            for v in pts_tri:
                d = np.sum((pts - v)**2, axis=1)
                idxs.append(int(np.argmin(d)))
            if len(set(idxs)) == 3:
                triangles.append(idxs)
        return np.array(triangles, dtype=np.int32)

def clamp_points(pts, w, h):
    out = pts.copy()
    out[:,0] = np.clip(out[:,0], 0, w-1)
    out[:,1] = np.clip(out[:,1], 0, h-1)
    return out

# ---------- cahnege distances ----------
S1_pairs_1b = [(38,42), (39,41)]                                   # d1, d2  (right eye)
S2_pairs_1b = [(44,48), (45,47)]                                   # d3, d4  (left eye)
S3_pairs_1b = [(50,61), (54,65), (62,68), (63,67), (64,66)]        # d5..d9  (lip inner)
S4_pairs_1b = [(50,60), (51,59), (52,58), (53,57), (54,56)]        # d10..d14 (lip outer)

S1 = [one_based_pair(*p) for p in S1_pairs_1b]
S2 = [one_based_pair(*p) for p in S2_pairs_1b]
S3 = [one_based_pair(*p) for p in S3_pairs_1b]
S4 = [one_based_pair(*p) for p in S4_pairs_1b]

names_S1 = ["d1","d2"]
names_S2 = ["d3","d4"]
names_S3 = ["d5","d6","d7","d8","d9"]
names_S4 = ["d10","d11","d12","d13","d14"]

pair_set_map = {**{k:"S1" for k in names_S1},
                **{k:"S2" for k in names_S2},
                **{k:"S3" for k in names_S3},
                **{k:"S4" for k in names_S4}}

# ---------- reduction factors (edit to taste) ----------
factors = {
    "d1": 0.95, "d2": 0.95,
    "d3": 0.95, "d4": 0.95,
    "d5": 0.65, "d6": 0.65, "d7": 0.65, "d8": 0.65, "d9": 0.65,
    "d10": 0.85, "d11": 0.85, "d12": 0.85, "d13": 0.85, "d14": 0.85,
}

# ---------- detect landmarks ----------
predictor = ensure_shape_predictor(PREDICTOR_PATH)
detector = dlib.get_frontal_face_detector()

img_bgr = cv2.imread(IMAGE_PATH)
if img_bgr is None:
    raise FileNotFoundError(IMAGE_PATH)
h_img, w_img = img_bgr.shape[:2]
gray = cv2.cvtColor(img_bgr, cv2.COLOR_BGR2GRAY)

rects = detector(gray, 1)
if not rects:
    raise RuntimeError("No face detected.")
rect = max(rects, key=lambda r: r.width()*r.height())

shape = predictor(gray, rect)
if shape.num_parts != 68:
    raise RuntimeError(f"Expected 68 landmarks, got {shape.num_parts}")
pts = shape_to_np(shape)  # (68,2) float32

# ---------- original distances ----------
d_orig = {}
d_orig.update(pair_distances(pts, S1, names_S1))
d_orig.update(pair_distances(pts, S2, names_S2))
d_orig.update(pair_distances(pts, S3, names_S3))
d_orig.update(pair_distances(pts, S4, names_S4))

# ---------- construct conditioned target landmarks ----------
disp_sum, disp_w = accumulate_displacements(68)
for (pairs, names) in [(S1, names_S1), (S2, names_S2), (S3, names_S3), (S4, names_S4)]:
    for (i, j), nm in zip(pairs, names):
        shrink_pair(pts, i, j, factors[nm], disp_sum, disp_w)

pts_cond = apply_displacements(pts, disp_sum, disp_w)

# add downward shift to mouth corners (49 & 55 in 1-based -> 48 & 54 in 0-based)
right_eye_idx = list(range(36, 42))
left_eye_idx  = list(range(42, 48))
center_right = pts[right_eye_idx].mean(axis=0)
center_left  = pts[left_eye_idx].mean(axis=0)
inter_ocular = float(np.linalg.norm(center_right - center_left))
down = 0.06 * inter_ocular  # tweak this strength if needed
for corner_idx in (48, 54):
    pts_cond[corner_idx, 1] += down


pts = clamp_points(pts, w_img, h_img)
pts_cond = clamp_points(pts_cond, w_img, h_img)


triangles = delaunay_triangles(pts)   # (M,3) indices
warped_bgr = triangle_warp(img_bgr, pts.astype(np.float32), pts_cond.astype(np.float32), triangles)
cv2.imwrite(OUT_IMG, warped_bgr)

# ---------- new distances update to image.............
d_new = {}
d_new.update(pair_distances(pts_cond, S1, names_S1))
d_new.update(pair_distances(pts_cond, S2, names_S2))
d_new.update(pair_distances(pts_cond, S3, names_S3))
d_new.update(pair_distances(pts_cond, S4, names_S4))

rows = []
for nm in ["d1","d2","d3","d4","d5","d6","d7","d8","d9","d10","d11","d12","d13","d14"]:
    rows.append({
        "Notation": nm,
        "Set": pair_set_map[nm],
        "Orig_Distance": d_orig[nm],
        "Target_Factor": factors[nm],
        "Expected_New": d_orig[nm] * factors[nm],
        "Actual_New": d_new[nm],
        "Delta_vs_Expected": d_new[nm] - d_orig[nm] * factors[nm],
    })
pd.DataFrame(rows).to_csv(OUT_DISTS_CSV, index=False)

# ---------- save custom 32 landmarks (face contour + eyes + nose anchors) ----------
CUSTOM_32_IDX = list(range(0,17)) + list(range(36,42)) + list(range(42,48)) + [30,31,35]
assert len(CUSTOM_32_IDX) == 32
row32 = {}
for idx in CUSTOM_32_IDX:
    x, y = pts_cond[idx]
    row32[f"x_{idx}"] = int(round(x))
    row32[f"y_{idx}"] = int(round(y))
row32["image"] = Path(IMAGE_PATH).name
pd.DataFrame([row32]).to_csv(OUT_32_CSV, index=False)

print("Saved:")
print(" - changed image:", OUT_IMG)
print(" - Distance report:", OUT_DISTS_CSV)
print(" - Custom 32 landmarks:", OUT_32_CSV)