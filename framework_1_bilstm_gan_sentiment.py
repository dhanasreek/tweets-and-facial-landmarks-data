# -*- coding: utf-8 -*-
"""Framework 1: BiLSTM_GAN_Sentiment.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JYn_iK6AhMczGc5KFIz4kX3Aq1Gwugx1
"""

from google.colab import drive
drive.mount('/content/drive')

"""## 1) Configuration"""

import os
import re
import math
import json
import random
import collections
from typing import List, Tuple, Dict

import numpy as np
import pandas as pd

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader

try:
    from sklearn.model_selection import train_test_split
except Exception:
    train_test_split = None

try:
    from sklearn.metrics import classification_report, accuracy_score
except Exception:
    classification_report = None
    accuracy_score = None

# Config
CSV_PATH ="/content/drive/MyDrive/facial lanadmarks/accident_tweets_with_sentiment_label-.csv"

SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

# Data / Vocab
MIN_FREQ = 2
MAX_LEN = 64

# Model
EMBED_DIM = 128
HIDDEN_DIM = 192
NUM_LAYERS = 1
DROPOUT = 0.2

# Training
BATCH_SIZE = 64
EPOCHS = 15
LR_G = 2e-3
LR_D = 2e-3

# Masking (L/K masking): mask_ratio = L / K
MASK_RATIO = 0.2  # e.g., L/K = 0.2

# Loss mixing
ALPHA_RECON = 1.0   # generator reconstruction (masked tokens only)
BETA_G_CLS  = 0.5   # generator auxiliary class loss weight
GAMMA_D_CLS = 1.0   # discriminator class loss weight

print(f"Using device: {DEVICE}")

"""## 2) Text Cleaning, Tokenization, and Vocab"""

def clean_text(s: str) -> str:
    if not isinstance(s, str):
        s = "" if s is None else str(s)
    s = s.lower()
    s = re.sub(r"http\S+|www\.\S+", " ", s)     # URLs
    s = re.sub(r"@\w+", " ", s)                 # mentions
    s = re.sub(r"#", "", s)                     # hashtags -> keep word
    s = re.sub(r"[^a-z0-9\s\.\,\!\?\:\;\-\'\"]+", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize(s: str) -> List[str]:
    s = re.sub(r"([.,!?;:\"\'\-])", r" \1 ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s.split(" ") if s else []

class Vocab:
    def __init__(self, min_freq=1, specials=None):
        self.counter = collections.Counter()
        self.min_freq = min_freq
        self.specials = specials or []
        self.itos = []
        self.stoi = {}

    def build(self, token_lists: List[List[str]]):
        for tl in token_lists:
            self.counter.update(tl)
        self.itos = list(self.specials)
        for tok, freq in self.counter.most_common():
            if freq >= self.min_freq and tok not in self.itos:
                self.itos.append(tok)
        self.stoi = {t:i for i,t in enumerate(self.itos)}

    def __len__(self): return len(self.itos)

    def encode(self, tokens: List[str]) -> List[int]:
        unk = self.stoi.get("[UNK]", 1)
        return [self.stoi.get(t, unk) for t in tokens]

    def decode(self, ids: List[int]) -> List[str]:
        return [self.itos[i] if 0 <= i < len(self.itos) else "[UNK]" for i in ids]

def pad_to_len(seq: List[int], max_len: int, pad_idx: int = 0) -> List[int]:
    if len(seq) >= max_len:
        return seq[:max_len]
    return seq + [pad_idx] * (max_len - len(seq))

"""## 3) Dataset and L/K Masking"""

def auto_pick_columns(df: pd.DataFrame) -> Tuple[str, str]:
    text_candidates = ["tweet", "text", "Tweets", "message", "content", "Tweet", "Text"]
    label_candidates = ["sentiment", "label", "polarity", "Sentiment", "Label"]
    text_col = None; label_col = None
    cols = set(df.columns)
    for c in text_candidates:
        if c in cols: text_col = c; break
    for c in label_candidates:
        if c in cols: label_col = c; break
    if text_col is None or label_col is None:
        raise ValueError(f"Could not auto-detect text/sentiment columns. Columns found: {list(df.columns)}")
    return text_col, label_col

def encode_labels(series: pd.Series) -> Tuple[np.ndarray, Dict[str,int], Dict[int,str]]:
    norm = series.astype(str).str.strip().str.lower().replace({
        "pos":"positive","neg":"negative","neu":"neutral","1":"positive","-1":"negative","0":"neutral"
    })
    uniq = sorted(norm.unique())
    label2id = {lbl:i for i,lbl in enumerate(uniq)}
    id2label = {i:lbl for lbl,i in label2id.items()}
    y = norm.map(label2id).values
    return y, label2id, id2label

class TweetDataset(Dataset):
    def __init__(self, X_ids: np.ndarray, y: np.ndarray, pad_idx: int, mask_idx: int, mask_ratio: float):
        self.X_ids = X_ids
        self.y = y
        self.pad_idx = pad_idx
        self.mask_idx = mask_idx
        self.mask_ratio = mask_ratio

    def __len__(self): return len(self.X_ids)

    def _make_lk_mask(self, seq: torch.Tensor) -> torch.Tensor:
        tokens = (seq != self.pad_idx).nonzero(as_tuple=False).view(-1)
        n = tokens.numel()
        if n == 0:
            return torch.zeros_like(seq, dtype=torch.bool)
        k = max(1, int(round(self.mask_ratio * n)))
        idx = tokens[torch.randperm(n)[:k]]
        mask = torch.zeros_like(seq, dtype=torch.bool)
        mask[idx] = True
        return mask

    def __getitem__(self, idx):
        ids = torch.tensor(self.X_ids[idx], dtype=torch.long)
        label = int(self.y[idx])
        mask = self._make_lk_mask(ids)
        masked_ids = ids.clone(); masked_ids[mask] = self.mask_idx
        return ids, masked_ids, mask, label

def collate(batch):
    ids = torch.stack([b[0] for b in batch], dim=0)
    masked_ids = torch.stack([b[1] for b in batch], dim=0)
    masks = torch.stack([b[2] for b in batch], dim=0)
    labels = torch.tensor([b[3] for b in batch], dtype=torch.long)
    return ids, masked_ids, masks, labels

"""## 4) Models: BiLSTM Generator and Discriminator"""

class Generator(nn.Module):
    def __init__(self, vocab_size, embed_dim, hidden_dim, num_layers=1, dropout=0.1, pad_idx=0):
        super().__init__()
        self.pad_idx = pad_idx
        self.embed = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=0.0 if num_layers==1 else dropout)
        self.dropout = nn.Dropout(dropout)
        self.out = nn.Linear(hidden_dim * 2, vocab_size)

    def forward(self, input_ids, noise=None):
        x = self.embed(input_ids)
        if noise is not None:
            if noise.dim() == 2:
                noise = noise.unsqueeze(1).expand(-1, x.size(1), -1)
            x = x + noise
        h, _ = self.lstm(x)
        h = self.dropout(h)
        logits = self.out(h)
        return logits

    @property
    def embedding_weight(self):
        return self.embed.weight

class Discriminator(nn.Module):
    def __init__(self, embed_dim, hidden_dim, num_classes, num_layers=1, dropout=0.2):
        super().__init__()
        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, bidirectional=True, batch_first=True, dropout=0.0 if num_layers==1 else dropout)
        self.dropout = nn.Dropout(dropout)
        self.adv = nn.Linear(hidden_dim * 2, 1)
        self.cls = nn.Linear(hidden_dim * 2, num_classes)

    def forward(self, seq_emb, mask_pad=None):
        h, _ = self.lstm(seq_emb)
        h = self.dropout(h)
        if mask_pad is not None:
            lengths = (~mask_pad).sum(dim=1).clamp(min=1).unsqueeze(-1)
            h_sum = (h * (~mask_pad).unsqueeze(-1)).sum(dim=1)
            pooled = h_sum / lengths
        else:
            pooled = h.mean(dim=1)
        adv_logit = self.adv(pooled).squeeze(-1)
        cls_logits = self.cls(pooled)
        return adv_logit, cls_logits

"""## 5) Training"""

def embed_ids_with(generator: Generator, ids: torch.Tensor, use_soft: bool, logits: torch.Tensor = None):
    if not use_soft:
        return generator.embed(ids)
    probs = F.softmax(logits, dim=-1)                # [B,T,V]
    emb_weight = generator.embedding_weight          # [V,E]
    soft_emb = torch.matmul(probs, emb_weight)       # [B,T,E]
    return soft_emb

def evaluate_classifier(D, G, loader, id2label, verbose=False):
    D.eval(); G.eval()
    total, correct = 0, 0
    all_true, all_pred = [], []
    with torch.no_grad():
        for ids, masked_ids, masks_lk, labels in loader:
            ids = ids.to(DEVICE); labels = labels.to(DEVICE)
            real_emb = G.embed(ids)
            pad_idx = getattr(G, "pad_idx", 0)
            pad_mask = (ids == pad_idx)
            _, cls_logits = D(real_emb, pad_mask)
            preds = cls_logits.argmax(dim=-1)
            total += labels.size(0)
            correct += (preds == labels).sum().item()
            if verbose:
                all_true.extend(labels.tolist()); all_pred.extend(preds.tolist())
    acc = correct / max(1, total)
    if verbose and classification_report is not None:
        target_names = [id2label[i] for i in sorted(id2label.keys())]
        print(classification_report(all_true, all_pred, target_names=target_names))
    return acc

"""## 6) Train & Validate"""

def train(
    df: pd.DataFrame,
    text_col: str,
    label_col: str,
    mask_ratio: float = MASK_RATIO,
    max_len: int = MAX_LEN,
):
    texts = df[text_col].astype(str).map(clean_text).tolist()
    toks = [tokenize(t) for t in texts]

    y, label2id, id2label = encode_labels(df[label_col])
    num_classes = len(label2id)

    if train_test_split is not None:
        X_train, X_tmp, y_train, y_tmp = train_test_split(toks, y, test_size=0.2, random_state=SEED, stratify=y)
        X_val, X_test, y_val, y_test = train_test_split(X_tmp, y_tmp, test_size=0.5, random_state=SEED, stratify=y_tmp)
    else:
        n = len(toks); idx = list(range(n)); random.shuffle(idx)
        tr = int(0.8*n); va = tr + int(0.1*n)
        X_train = [toks[i] for i in idx[:tr]]; y_train = y[idx[:tr]]
        X_val   = [toks[i] for i in idx[tr:va]]; y_val = y[idx[tr:va]]
        X_test  = [toks[i] for i in idx[va:]]; y_test = y[idx[va:]]

    specials = ["[PAD]","[UNK]","[MASK]"]
    vocab = Vocab(min_freq=MIN_FREQ, specials=specials)
    vocab.build(X_train)

    pad_idx = vocab.stoi["[PAD]"]; mask_idx = vocab.stoi["[MASK]"]

    def to_ids(token_lists):
        arr = []
        for tl in token_lists:
            ids = vocab.encode(tl)
            arr.append(pad_to_len(ids, max_len, pad_idx))
        return np.array(arr, dtype=np.int64)

    Xtr_ids = to_ids(X_train); Xva_ids = to_ids(X_val); Xte_ids = to_ids(X_test)

    train_ds = TweetDataset(Xtr_ids, y_train, pad_idx, mask_idx, mask_ratio)
    val_ds   = TweetDataset(Xva_ids, y_val, pad_idx, mask_idx, mask_ratio=0.0)
    test_ds  = TweetDataset(Xte_ids, y_test, pad_idx, mask_idx, mask_ratio=0.0)

    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate)
    val_loader   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)
    test_loader  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate)

    vocab_size = len(vocab)
    G = Generator(vocab_size, EMBED_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, pad_idx).to(DEVICE)
    D = Discriminator(EMBED_DIM, HIDDEN_DIM, num_classes=num_classes, num_layers=NUM_LAYERS, dropout=DROPOUT).to(DEVICE)

    opt_G = torch.optim.AdamW(G.parameters(), lr=LR_G)
    opt_D = torch.optim.AdamW(D.parameters(), lr=LR_D)

    bce = nn.BCEWithLogitsLoss()
    ce  = nn.CrossEntropyLoss()

    def make_pad_mask(ids_batch):
        return (ids_batch == pad_idx)

    for epoch in range(1, EPOCHS + 1):
        G.train(); D.train()
        total_d, total_g, n_batches = 0.0, 0.0, 0

        for ids, masked_ids, masks_lk, labels in train_loader:
            ids = ids.to(DEVICE); masked_ids = masked_ids.to(DEVICE)
            masks_lk = masks_lk.to(DEVICE); labels = labels.to(DEVICE)

            # ---- Train D ----
            opt_D.zero_grad()
            real_emb = G.embed(ids)
            adv_real_logit, cls_real_logits = D(real_emb, make_pad_mask(ids))

            noise = torch.randn(masked_ids.size(0), EMBED_DIM, device=DEVICE)
            gen_logits = G(masked_ids, noise=noise)
            fake_emb = embed_ids_with(G, masked_ids, use_soft=True, logits=gen_logits)
            adv_fake_logit, cls_fake_logits = D(fake_emb, make_pad_mask(masked_ids))

            # Eq.14 adversarial loss for D
            d_real = bce(adv_real_logit, torch.ones_like(adv_real_logit))
            d_fake = bce(adv_fake_logit, torch.zeros_like(adv_fake_logit))
            loss_d_adv = d_real + d_fake

            loss_d_cls = ce(cls_real_logits, labels)  # ACGAN real-class loss

            loss_D = loss_d_adv + GAMMA_D_CLS * loss_d_cls
            loss_D.backward(); opt_D.step()

            # ---- Train G ----
            opt_G.zero_grad()
            noise = torch.randn(masked_ids.size(0), EMBED_DIM, device=DEVICE)
            gen_logits = G(masked_ids, noise=noise)
            fake_emb = embed_ids_with(G, masked_ids, use_soft=True, logits=gen_logits)
            adv_fake_logit, cls_fake_logits = D(fake_emb, make_pad_mask(masked_ids))

            # Eq.14 adversarial loss for G
            loss_g_adv = bce(adv_fake_logit, torch.ones_like(adv_fake_logit))

            # Reconstruction CE on masked tokens only
            B,T,V = gen_logits.size()
            gen_logits_flat = gen_logits.view(B*T, V)
            target_flat = ids.view(B*T)
            mask_flat = masks_lk.view(B*T)
            if mask_flat.any():
                loss_recon = ce(gen_logits_flat[mask_flat], target_flat[mask_flat])
            else:
                loss_recon = torch.tensor(0.0, device=DEVICE)

            loss_g_cls = ce(cls_fake_logits, labels)  # encourage class-consistent generations

            loss_G = loss_g_adv + ALPHA_RECON * loss_recon + BETA_G_CLS * loss_g_cls
            loss_G.backward(); opt_G.step()

            total_d += loss_D.item(); total_g += loss_G.item(); n_batches += 1

        print(f"Epoch {epoch:02d}/{EPOCHS} | D_loss={total_d/n_batches:.4f} | G_loss={total_g/n_batches:.4f}")
        val_acc = evaluate_classifier(D, G, val_loader, id2label)
        print(f"  Val Acc: {val_acc:.4f}")

    test_acc = evaluate_classifier(D, G, test_loader, id2label, verbose=True)
    print(f"Test Acc: {test_acc:.4f}")

    os.makedirs("./checkpoints", exist_ok=True)
    torch.save(G.state_dict(), "./checkpoints/generator.pt")
    torch.save(D.state_dict(), "./checkpoints/discriminator.pt")
    with open("./checkpoints/meta.json","w") as f:
        json.dump({
            "label2id": {k:int(v) for k,v in {k:i for i,k in enumerate(sorted(set(df[label_col].astype(str).str.lower())))}.items()},
            "id2label": {i:lbl for i,lbl in enumerate(sorted(set(df[label_col].astype(str).str.lower())))},
            "vocab_itos": vocab.itos,
            "config": {"EMBED_DIM": EMBED_DIM, "HIDDEN_DIM": HIDDEN_DIM, "MAX_LEN": max_len, "MASK_RATIO": mask_ratio}
        }, f, indent=2)
    print("Saved checkpoints to ./checkpoints")

    return G, D, vocab, id2label

"""## 7) Load CSV and Train"""

assert os.path.exists(CSV_PATH), f"CSV not found at {CSV_PATH}"
df = pd.read_csv(CSV_PATH)
text_col, label_col = auto_pick_columns(df)
print(f"Detected columns -> text: '{text_col}', sentiment: '{label_col}'")
G, D, vocab, id2label = train(df, text_col, label_col, mask_ratio=MASK_RATIO, max_len=MAX_LEN)

"""## 8)  Predict Sentiment Labels"""

def predict_sentiments(texts: List[str], G: Generator, D: Discriminator, vocab_itos: List[str], id2label: Dict[int,str], max_len: int = MAX_LEN) -> List[str]:
    stoi = {t:i for i,t in enumerate(vocab_itos)}
    pad_idx = stoi.get("[PAD]", 0); unk_idx = stoi.get("[UNK]", 1)

    def encode_one(s: str):
        s = clean_text(s); tl = tokenize(s)
        ids = [stoi.get(t, unk_idx) for t in tl]
        ids = pad_to_len(ids, max_len, pad_idx)
        return torch.tensor(ids, dtype=torch.long)

    with torch.no_grad():
        D.eval(); G.eval()
        batch = torch.stack([encode_one(s) for s in texts], dim=0).to(DEVICE)
        emb = G.embed(batch)
        pad_mask = (batch == pad_idx)
        _, cls_logits = D(emb, pad_mask)
        preds = cls_logits.argmax(dim=-1).tolist()
        return [id2label[i] for i in preds]

# test
test = [
    "I love how quickly the issue was resolved!",
    "Terrible service, never coming back.",
    "It was okay, nothing special."
]
try:
    preds = predict_sentiments(test, G, D, vocab.itos, id2label, max_len=MAX_LEN)
    for s, p in zip(test, preds):
        print(f"[{p}] {s}")
except Exception as e:
    print("Prediction test skipped due to error:", e)